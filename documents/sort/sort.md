#### 基于比较的排序算法 ####

总结一下基本的排序算法，以及较为深入地研究一下复杂度情况和实现的难点以及问题。下面讨论中都假设 A\[n\]为输入数组，目标数组为升序（从小到大）。本文中的代码都为伪代码。

值得补充的是除了注意算法的时间复杂度、空间复杂度和是否是原址排序之外，我们很容易忽略的是排序算法的 **稳定性**，其定义为 当 $A_i = A_j$，排序之前 $A_i$ 在 $A_j$ 之前时，如果排序之后  $A_i$ 仍在 $A_j$ 之前，那么称这种算法是稳定的。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。

对于不稳定的排序算法，只要举出一个实例，即可说明它的不稳定性；而对于稳定的排序算法，必须对算法进行分析从而得到稳定的特性。需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法（比如把小于改成小于等于或反过来）。排序算法稳定的好处：排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，前一个键排序的结果可以为后一个键排序所用。

> PS：简单的入门级排序算法，即冒牌排序、选择排序略

##### 插入排序 #####

插入排序一般实现为 **原址排序**（在任何时候，最多只有其中常数个数元素存储在数组外），可以理解为将每个 A[j] 插入到 A[0 : j-1] 的子数组中去：

```
// 数据结构 ---------- 数组
// 最差时间复杂度 ---- 最坏情况为输入序列是降序排列的,此时时间复杂度O(n^2)
// 最优时间复杂度 ---- 最好情况为输入序列是升序排列的,此时时间复杂度O(n)
// 平均时间复杂度 ---- O(n^2)
// 所需辅助空间 ------ O(1)
// 稳定性 ------------ 稳定


insertion_sort(A):
	for j = 1 to A.length-1:
		key = A[j]
		i = j-1
		
		//此时子数组A[0:j-1]已排序，只要元素大于key就将其后移，小于等于就停止，此时i+1即为目标位置 
		while i >= 0 and A[i] > key:
    		A[i+1] = A[i]  //另一种思路是将这个地方换成 swap(A[i+1], A[i])，最后一行去掉，三次赋						   //值，交换法可能会稍慢
    		i = i - 1
    	A[i+1] = key
```

该算法时间复杂度为 O($n^2$)，因为要移动元素。当比较操作所需的消耗较大时，我们可以采用二分查找来优化插入排序，叫做**二分插入排序**（因为 A[0 : j-1] 是有序的）：

```
// 数据结构 ---------- 数组
// 最差时间复杂度 ---- O(n^2)
// 最优时间复杂度 ---- O(nlogn)
// 平均时间复杂度 ---- O(n^2)
// 所需辅助空间 ------ O(1)
// 稳定性 ------------ 稳定


insertion_sort(A):
	for j = 1 to A.length-1:
		key = A[j]
		end = j-1               //[end,start]表示二分搜索的子（闭）区间 
		start = 0
		while end != start:
			i = (end+start)/2   
			if A[i] > key:
				end = i         //不能end=i-1，当end-start=1且A[start]>key时会出现错误
			else:
				start = i+1     //要注意是地板除！！当end-start=1且A[start]<=key时，如果
							    //此处为 start = i 可能会死循环。
		while p = j-1 to end:
			A[p+1] = A[p]

    	A[end] = key            //循环结束后A[start]==A[end]>key
```

> PS：二分查找的细节要注意，如上述例子伪代码的循环中只能 end = i，start = i + 1，考虑end和start相邻的特殊情况可得

此时（总共）需要 O($n\lg{n}$) 次比较操作。但是实际的插入排序总时间复杂度仍然是 O($n^2$)。（二分）插入排序是稳定的。

由于**插入排序算法在对几乎已经排好序的数据进行操作时，可以达到线性的效率**。基于这个特性，还有一种更高效的改进叫做**希尔排序**，希尔排序的基本思路是通过某个增量（伪代码中的gap）将数组元素划分为若干组（同一组中的相邻元素索引之差等于此时所选增量），然后分组进行插入排序，随后逐步缩小增量，继续按组进行插入排序操作，直至增量为1：

```
// 数据结构 ---------- 数组
// 最差时间复杂度 ---- 根据步长序列的不同而不同。已知最好的为O(n(logn)^2)
// 最优时间复杂度 ---- O(n)
// 平均时间复杂度 ---- 根据步长序列的不同而不同。
// 所需辅助空间 ------ O(1)
// 稳定性 ------------ 不稳定


shell_sort(A):
	gap = A.length/2                 //增量等于A.length/2是希尔建议的增量，但不是最优的
	while gap > 0:
		for i = gap to A.length-1:   //从gap开始更科学，A[0:gap-1]是每组的第一个元素
			key = A[i]
			pre = i-gap
			while pre >= 0 and A[pre] > key:       //!!条件判断的写法，pre要大于等于0
				A[pre+gap] = A[pre]
				pre = pre-gap
			A[pre+gap] = key
		gap = gap/2
```

希尔排序是**不稳定的**。虽然一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱。

##### 归并排序 #####

归并排序是 **分治思想** 的一个重要运用。归并排序的大概思路是将一个大数组分解成两个已排序的子数组，然后将两个已排序的子数组合并。（当子数组中元素只有一个时显然是已排序的）在子问题还不是很小时，我们只是不断分解问题，最后不断合并子问题的解，得到目标解，因此，几乎所有操作都是合并操作。（如果用原址的合并操作的话需要移动元素，合并操作复杂度为 O($n^2$)... 所以一般不用原址的归并排序，而是使用额外的空间辅助合并）此时合并操作是 O($n$) 的，所以总时间复杂度为 O($n\lg{n}$)：

> PS：计算分治算法时间复杂度时需要把**递归树**画出来进行计算（高度为 $\lg{n} + 1$，将**每个节点**消耗的工作量加起来就是时间复杂度。要注意表达式中的**常数项**，因为叶子节点数为n，当为二叉树时，总节点数为2n-1，所以常数项对复杂度的贡献总为 O($n$)），不然很容易算错，用 T(n) 表示n个输入做的工作。比如说： T(n) = 2T($n/2$) + $c*n^2$ 的时间复杂度就不是 O($n^2\lg{n}$) 而是 O($n^2$)，而 T(n) = 2T($n/2$) + $c$ 的时间复杂度是 O($n$)，而不是 O($n\lg{n}$)。

```
// 数据结构 ---------- 数组
// 最差时间复杂度 ---- O(nlogn)
// 最优时间复杂度 ---- O(nlogn)
// 平均时间复杂度 ---- O(nlogn)
// 所需辅助空间 ------ O(n)
// 稳定性 ------------ 稳定


//A为需要排序的数组，合并两个已排序的数组 A[p:q] 和 A[q+1:r]，L[q-p+1]、R[r-q]为辅助数组
merge(A, p, q, r)：
	n1 = q-p
	n2 = r-q-1
	for i = 0 to n1:
		L[i] = A[p+i]
	for i = 0 to n2:
		R[i] = A[q+1+i]
	left = right = 0
	for position = p to r:             //left和right是两只手指（MIT 6.006）
		if L[left] < R[right]:
			A[position] = L[left]
			left = left + 1
		else:
			A[position] = R[right]
			right = right + 1
		
//A为需要排序的数组，需要排序的区间为[p, r]
merge_sort(A, p, r):
	middle = (p+r)/2
	if middle == r:     //p==r的情况
		return 
	merge_sort(A, p, middle)
	merge_sort(A, middle+1, r)
	merge(A, p, middle, r)
```

因为可以直接分配一个较大（和A一样大）的辅助空间满足需求，所以空间复杂度为 O($n$)。归并排序是稳定的。

> PS：一些算法时间复杂度常数系数较小，但复杂度指数较大，所以在数据量较小的情况下可能表现地更好，比如说归并排序复杂度的常数系数就比插入排序的大。

##### 堆排序 #####

堆排序基于堆（heap）的数据结构（见 documents/heap/heap.sort）。为原址排序，时间复杂度为 O($n\lg{n}$)。基本思路是：先将输入的无序数组 A 建立成一个最大堆，然后将堆顶元素（最大值）取出放在数组末尾，忽略取出元素后的数组继续维护成堆。不断重复上述操作：

```
// 数据结构 ---------- 数组
// 最差时间复杂度 ---- O(nlogn)
// 最优时间复杂度 ---- O(nlogn)
// 平均时间复杂度 ---- O(nlogn)
// 所需辅助空间 ------ O(1)
// 稳定性 ------------ 不稳定


//堆操作函数的伪代码见 documents/heap/heap.sort
heap_sort(A):
	build_heap(A)                //时间复杂度为 O(n)
 	for p = A.length-1 to 1:
		swap(A[0], A[p])
		heapify(A, 0, p)         //时间复杂度为 O(lg n)
```

堆排序是**不稳定的**。不稳定发生在堆顶元素与数组末尾元素交换的时候。

##### 快速排序 #####

快速排序也是基于分治思想的一种算法，它也是原址排序的。基本思路是：先从数组中选择一个元素作为基准，根据基准将数组分为两个区间，将比基准小的放在基准的左边，将比基准大的放在基准的右边。然后在每个部分重复上述操作：

```
// 数据结构 --------- 数组
// 最差时间复杂度 ---- 每次选取的基准都是最大（或最小）的元素，导致每次只划分出了一个分区，需要进行n-1次划分才能结束递归，时间复杂度为O(n^2)
// 最优时间复杂度 ---- 每次选取的基准都是中位数，这样每次都均匀的划分出两个分区，只需要logn次划分就能结束递归，时间复杂度为O(nlogn)
// 平均时间复杂度 ---- O(nlogn)
// 所需辅助空间 ------ 主要是递归造成的栈空间的使用(用来保存left和right等局部变量)，取决于递归树的深度，一般为O(logn)，最差为O(n)       
// 稳定性 ---------- 不稳定


//这里我们简单地选择每个区间最后一个元素作为基准，A为输入数组，p为首元素索引，q为最后一个元素的索引
quick_sort(A, p, q):
	border = partition(A, p, q)
	quick_sort(A, p, border-1)
	quick_sort(A, border+1, q)   //每个作为基准的元素的位置之后不会再变化

//r为基准元素的索引，A[p:q]为要分区的区间
partition(A, p, q, r):
	swap(A[r], A[q])
	base = A[q]
	i = p-1                      //一开始还没有“看”到小于基准元素的元素
	for j = p to q-1:
		if A[j] < base:
			i = i + 1            //i位置以及其之前的元素都小于等于基准元素，i总为两区间边界
			swap(A[j], A[i])     //j遍历整个数组，将小于基准元素的元素交换到i处
	swap(A[i+1], A[q])
	return i+1
```

快排的最坏情况为 O($n^2$)，发生在每次划分的两个子区间分别包含了 n-1 和 0 个元素的时候，这种情况下，在每一层递归上，划分都是最大程度不平衡的。但是快速排序的**平均性能非常好**，任何一种常数比例的子问题规模划分（即使是 99:1） 都会产生 O($\lg{n}$) 的时间复杂度（可以画出递归树来证明）。并且快排复杂度中的常数因子很小，所以快速排序通常是实际排序应用中最好的选择。（可以用基于随机抽样的快速排序算法来避免最坏情况的发生）

快速排序是**不稳定的**，不稳定发生在基准元素与 A[i+1] 元素交换时。考虑到不稳定性，有时会有别的稳定的算法，而不是用不稳定的快速排序。

 由此可见，对于基于比较的算法来说，我们能达到的最好的搜索操作的时间复杂度为 O($\lg{n}$)，最好的排序操作的时间复杂度为 O($n\lg{n}$)。（也可以用比较操作的决策树dicision tree来证明，一个比较操作的结果只会有两个，树的叶子节点必须涵盖所有n种情况）

#### 基于Random Access Machine的排序算法 ####

Random Access Machine的典型例子是数组，我们访问其中已知位置的任意一个元素的复杂度为常数 O(1)。跟hash思想类似。在接下来讨论的排序中，我们只讨论排序整型（当然其他类型的元素也可以映射到整型）的元素。并且这些算法的时间复杂度为 O($n$)，即线性时间复杂度。

##### 计数排序 #####

计数排序的基本思路是用一个辅助数组空间来（在元素值作为索引的单元处）记录每个元素值出现的次数，然后遍历这个辅助数组空间按照非空单元的位置顺序将每个单元的索引输出对应次数（其中小于辅助数组中某个非空单元对应的元素的元素个数为它之前所有单元之和）。当然如果我们是以一个对象的某个属性作为 key 来对对象集合进行排序时，我们也可以直接将这个对象保存在辅助数组空间的单元中，用链表或其他的结构（决定了该算法是否是稳定的）：

```
// 数据结构 --------- 数组
// 最差时间复杂度 ---- O(n)
// 最优时间复杂度 ---- O(n)
// 平均时间复杂度 ---- O(n)
// 所需辅助空间 ------ 辅助空间的大小要看元素值的范围R，O(R)

//A为需要排序的对象的数组
//max表示A中的最大元素值
//output为排序后数组
count_sort(A, output):         
	L = array(max_of(A))                   //分配一个辅助的空间
	for item in A:
		L[key(item)].append(item)
	index = 0
	for item in L:
		if !item.empty():
			for i in item:
				output[index] = item
				index = index + 1
	return output
```

##### 基数排序 #####

我们可以将一个整数看做是一个任意（k）进制的值得到一串介于[0, k-1]的数字，基数排序（radix sort）的基本思想是，我们从低位数字开始，用每个元素的该位数字对每个元素进行*计数排序*（从低位开始因为地位权级较低），直到操作完每个元素的最高位数字（要适当选择基数，也就是数字位数，数字位数一般不大于$\lg{n}$，不然会比比较排序慢）：

```
// 数据结构 --------- 数组
// 最差时间复杂度 ---- O(n)
// 最优时间复杂度 ---- O(n)
// 平均时间复杂度 ---- O(n)
// 所需辅助空间 ------ 辅助空间的大小要看进制单位的范围k，O(k)

//A为待排序数组
//k为进制
//max_dn为数组中元素的最大位数
radix_sort(A, k, max_dn):
	L = array(k)                   //分配一个辅助的空间
	output = A.clone()             //output为输出数组
	for count in range(max_dn):
		for item in output:
			L[key(item)/(k**count)%k].append(item)
		index = 0
		for item in L:
			if !item.empty():
				for i in item:
					output[index] = item
					index = index + 1
		L.clear()
	return output
```

其稳定性取决于采用的计数排序算法。

##### 桶排序 #####

桶排序也叫箱排序。工作的原理是将数组元素映射到有限数量个桶里，利用*计数排序*可以定位桶的边界，每个桶再各自进行桶内排序（使用其它排序算法或以递归方式继续使用桶排序）。

桶排序不是比较排序，不受到O(nlogn)下限的影响，它是鸽巢排序的一种归纳结果，当所要排序的数组值**分散均匀**的时候，桶排序拥有线性的时间复杂度。（代码略，其稳定性受桶内部的排序算法影响）