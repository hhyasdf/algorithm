#### 散列表 ####

散列表（Hash table，也叫哈希表），是根据键（Key）直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录（一个记录是一个完整的需要存储的元素，包括作为键的部分），这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。

散列表通常和平衡二叉树一起用来构造关联数组（又称映射Map，字典dictionary），相对于AVL的时间复杂度为 O($\lg{n}$) 的搜索操作，在散列表中的搜索操作是 O(1) 的。

当我们想要实现一个hash table的时候，我们首先想到的肯定是用一个大的数组去存储元素，但是我们会遇到两个问题，首先，key不一定是非负整数；其次，当我们的key的范围比较大和分散时，我们需要一个无比巨大的数组来存储，然而我们的内存是有限的。

通常在hash之前，我们需要将不是非负整数的key处理成非负整数，通常叫做 pre-hash。所以作为key的值是不能改变的。然后，为了减少需要的数组空间，我们通过将每个作为key的整数（prehash后）通过hash函数映射到一个更小的范围（用来存储的数组的大小）。显然，由于范围变小，会带来一个问题：一个key可能会被映射到同一个单元，也叫作冲突（collusion）。

理想的情况下，我们可以将prehash之后的key从一个大的范围（中的一些离散值）映射到一个小范围（用来存储的hash table数组的大小）并且不产生冲突。但考虑我们不断插入（insert）key，**当元素的数量大于存储的数组大小时**，这样显然会破坏理想的情况，这种情况下我们可能会考虑扩张hash table（数组）的大小（**扩容**）。为了扩张hash table我们需要重新分配一个更大的数组、重新选择hash函数、并将原来的每一个key重新映射到新的数组上，整个操作是 O($n$) 的（n为重新分配的数组的大小）。这样带来了一个问题，我们应该选择怎样增长数组的大小呢？如果每次增长一个单元，那么增长操作的时间复杂度为 O($1+2+3+...+n$)，即 O($n^2$)，平均每次操作时间复杂度为 O($n$)，显然无法保证hash table所需要的常数操作时间；如果每次增长2倍，总时间复杂度为 O($1+2+4+...+n$)，（每次扩张的时间增加但扩张次数减小）即 O($n$)，平均每次操作时间复杂度为 O(1)，由此，每次**扩张hash table使其大小增长2倍**的时间复杂度更低（table doubling）。（底层用数组实现的动态结构如STL的vector、Python的list的增长都采用的table doubling）整个扩张也叫做**rehash**。

insert时使用table doubling也会带来两个问题，首先是可能会带来巨大的内存占用。为了减少hash table的内存占用，我们唯一能做的就是在delete时考虑减少空闲的内存，但如果我们简单地考虑当元素的数量小于hash table的1/2大小就分配一个1/2大小的数组并重新哈希，我们可能会在连续的插入删除操作中消耗大量的时间（每个resize操作都是O($n$)的，可能会不断resize），所以我们考虑**当元素的数量等于存储数组的1/4时将hash table的容量减少1/2**，这样就避免了连续的插入删除操作带来的问题，并且平均操作的时间复杂度是O($1$)的。

另外一个问题是可能在单个操作上会造成很慢的响应。为了解决这个问题，我们可以并发地使用一个线程进行分配内存和重新hash操作，并在resize完成之前仍然使用原来的数组存储（拉链法可以存储大于hash table大小的元素）。这样就可以加快响应。

解决冲突的方法有很多种，代表性的包括拉链法、开放定址法。

**拉链法**将映射到同一单元的所有元素保存在一个链表中（没有被映射到的单元是一个空链表），拉链法为搜索带来了额外的时间复杂度，其工作的好坏取决于哈希函数的随机性和hash table的大小（考虑元素的数量远大于hash table大小的情况）。最坏的情况是所有元素都映射到同一个单元，此时搜索的时间复杂度为 O($n$)。

**开放定址法**的前提条件是hash table的大小要大于等于元素的数量，因为我们完全不打算使用链表（也因此可以节省指针所占用的空间用来存储更多的元素），每个hash table单元只能保存一个元素。

开放定址发的基本思路如下：考虑insert操作，我们需要插入一个key，首先计算它应该插入的位置索引hash(key, 1)，然后将其插入目标索引的位置；当我们发现该位置已经存在了一个元素时，我们进行第二次哈希hash(key, 2)，也是开放定址法最重要的思想，探查（probe）。然后确定hash(key, 2)的索引位置上是否已经存在一个元素，如果不存在，则插入；如果存在，即继续下一次探查。然后我们需要考虑的是search操作，我们根据给定的key来搜索hash table中对应key的索引位置；同样，如果 hash(key, 1) 对应的位置上元素的key不等于目标key时，我们进行下一次探查，直到找到目标key或该次探查的位置为EMPTY。

这样的search也会带来一个问题，就是如果当我们的目标元素存储在hash(key, i)的位置，但是凑巧hash(key, i-1)被删除了，此时如果delete操作将该单元简单地置为EMPTY，我们将无法找到目标元素，即使它已经被存储在hash table中。所以，在delete操作中，我们需要引入一个DELETE标志，并设置被删除的元素所在的单元。对于DELETE标志，insert操作将其视为与EMPTY等价（重用空间），而search将其单独处理。

探查策略常用的有以下几种：

* 线性探查：$hash(key, i) = (hash\_origin(key) + i)\mod{m}$，即每次探查向后探查一个单元（hash_origin是我们采用的原始的hash函数，m为散列表的大小）。线性探查存在一个缺点就是随着不断地插入，元素可能集中在hash table的某些位置，且集中的元素会越来越多，而一但在该位置进行其他操作，则可能需要进行大量的探查（不再是常数时间复杂度的操作）。
* 二次探查：$hash(key, i) = (hash\_origin(key) + i^2)\mod{m}$ 。二次探查增加平方运算是为了不让元素都聚集在一起。
* 随机探测：$hash(key,i) = (hash\_origin(key) + d_i) \mod{m}$ 。（其中 $d_i$ 是一个伪随机数列，伪随机数是说，如果我们设置随机种子相同，则不断调用随机函数可以生成不会重复的数列，我们在査找时，用同样的随机种子，它每次得到的数列是相同的，相同的 $d_i$ 当然可以得到相同的散列地址）
* 双重哈希：$hash(key, i) = (hash_1(key) + i*hash_2(key)) \mod{m}$ 。（其中 $hash_1$ 和 $hash_2$ 为不同的hash函数） 

开放定址法存在一个性能问题，当元素个数增多时，所需的探查操作会增多，我们可能无法再保证 O(1) 的操作时间复杂度。所以一般当元素个数（有时也需要考虑DELETE标志的单元数加上元素的个数，因为DELETE标志对search操作的性能也有很大影响）等于hash table的大小的50%~60%时，我们需要对hash table进行rehash。需要注意一点是，rehash过程中我们可以将所有的DELETE忽略（视为EMPTY），因为我们重新进行了所有元素的映射。

> PS：可以证明插入操作的cost <= 1/(1-k)，其中k=n/m，n为元素个数，m为hash table的大小